"""
    generate_pipeline_template(output_file::String = "custom_pipeline.jl", function_name::String = "custom_preprocess")

Generate a template file for users to create their own preprocessing pipelines.
The template includes standard setup (logging, config loading, error handling) and
provides a structured framework with sections and subsections for custom processing.

# Arguments
- `output_file::String`: Path where the template file should be saved
- `function_name::String`: Name for the main preprocessing function
"""
function generate_pipeline_template(
    output_file::String = "custom_pipeline.jl",
    function_name::String = "custom_preprocess",
)
    template_content = """
# Custom EEG Preprocessing Pipeline Template
# Generated by eegfun.generate_pipeline_template()

using eegfun
"""
    # Add the main function template
    template_content *= """
\"\"\"
    $function_name(config::String; log_level::String = "info")

Custom preprocessing pipeline for EEG data.

# Arguments
- `config::String`: Path to the configuration file in TOML format
- `log_level::String`: Log level for preprocessing ("debug", "info", "warn", "error")
\"\"\"
function $function_name(config::String; log_level::String = "info")
    
    # Set up the global log for overall processing
    global_log = setup_global_logging("$function_name.log", log_level = log_level)
    
    # Initialize variables for outer scope
    output_directory = ""
    all_epoch_counts = DataFrame[]  # Vector to store all epoch counts
    
    try
        @info section("Setup")
        !isfile(config) && @minimal_error "Config file does not exist: \$config"
        cfg = load_config(config)
        cfg == nothing && @minimal_error "Failed to load configuration from: \$config"
        
        # Try and merge user config above with default config
        default_config = load_config(joinpath(@__DIR__, "..", "..", "src", "config", "default.toml"))
        default_config == nothing && @minimal_error "Failed to load default configuration"
        cfg = _merge_configs(default_config, cfg)
        
        # Create the PreprocessConfig object
        preprocess_cfg = PreprocessConfig(cfg["preprocess"])
        
        # Check if all requested raw data files exist
        raw_data_files = get_files(cfg["files"]["input"]["directory"], cfg["files"]["input"]["raw_data_files"])
        raw_data_files_exist = check_files_exist(raw_data_files)
        !raw_data_files_exist && @minimal_error "Missing raw data files requested within TOML file!"
        @info "Found \$(length(raw_data_files)) files: \$(join(raw_data_files, ", "))"
        
        # Read the epoch conditions defined within the toml file
        !isfile(cfg["files"]["input"]["epoch_condition_file"]) && @minimal_error "File missing: \$(cfg["files"]["input"]["epoch_condition_file"])"
        epoch_cfgs = parse_epoch_conditions(TOML.parsefile(cfg["files"]["input"]["epoch_condition_file"]))
        @info "Epoch file: \$(cfg["files"]["input"]["epoch_condition_file"]) loaded"
        
        # Find and load layout file
        layout_file = find_file(cfg["files"]["input"]["layout_file"], joinpath(@__DIR__, "..", "..", "data", "layouts"))
        layout_file === nothing && @minimal_error "Layout file not found: \$layout_name"
        layout = read_layout(layout_file)
        
        # Check if requested output directory exists and if not, create it
        output_directory = cfg["files"]["output"]["directory"]
        !isdir(output_directory) && mkpath(output_directory)
        
        # Print config to output directory
        print_config(cfg, joinpath(output_directory, "config.toml"))
        
        # Layout coordinates and calculation of channel neighbours (2D)
        get_layout_neighbours_xy!(layout, cfg["preprocess"]["layout"]["neighbour_criterion"])
        print_layout_neighbours(layout, joinpath(output_directory, "neighbours_xy.toml"))
        
        # ============================================================================
        # CUSTOM PREPROCESSING PIPELINE STARTS HERE
        # ============================================================================
        
        # Initialize counters
        processed_files = 0
        failed_files = String[]
        
        for data_file in raw_data_files
            try
                @info section("Processing")
                @info "File: \$data_file"
                
                # Set up per-file logging (temporarily replaces global logger)
                setup_logging(joinpath(output_directory, "\$(basename_without_ext(data_file)).log"), log_level = log_level)
                
                # ========================================================================
                # CUSTOM PROCESSING STEPS
                # ========================================================================
                
                @info section("Step 1")
                @info subsection("Step 1.1")
                # Your code here...
                
                @info subsection("Step 1.2")
                # Your code here...
                
                @info section("Step 2")
                @info subsection("Step 2.1")
                # Your code here...
                
                @info subsection("Step 2.2")
                # Your code here...
                
                @info section("Step 3")
                @info subsection("Step 3.1")
                # Your code here...
                
                @info subsection("Step 3.2")
                # Your code here...
                
                @info section("Step 4")
                @info subsection("Step 4.1")
                # Your code here...
                
                @info subsection("Step 4.2")
                # Your code here...
                
                @info section("Step 5")
                @info subsection("Step 5.1")
                # Your code here...
                
                @info subsection("Step 5.2")
                # Your code here...
                
                @info section("End of Processing")
                @info "Successfully processed \$data_file"
                processed_files += 1
                
            catch e
                @error "Error processing \$data_file: \$e"
                push!(failed_files, data_file)
            finally
                close_logging()
            end
        end
        
        # Write final summary
        @info section("Summary")
        @info "\$processed_files success, \$(length(failed_files)) fail"
        !isempty(failed_files) && @info "Failed files: \$(join(failed_files, ", "))"
        
    catch e
        @error "Fatal error in preprocessing pipeline: \$e"
        rethrow(e)
    finally
        close_global_logging()
    end
end

# ============================================================================
# USAGE EXAMPLE
# ============================================================================

# To use this custom pipeline:
# 1. Modify the sections above to add your specific processing steps
# 2. Update the configuration file as needed
# 3. Run with: $function_name("your_config.toml", log_level = "info")
"""

    # Write the template to file
    open(output_file, "w") do io
        write(io, template_content)
    end

    @info "Pipeline template generated: \$output_file"
    @info "Edit the template to add your custom processing steps"

    return output_file
end
